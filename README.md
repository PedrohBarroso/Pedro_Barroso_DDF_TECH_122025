# Case T√©cnico - Yellow Taxi (Processo Seletivo - Eng. Dados Junior DadosFera)
**Candidato:** Pedro Henrique Barroso
**Sistema Operacional:** Linux | **Linguagem:** Python, SQL | **Bibliotecas:** Pandas, Numpy, soda-Cora
**Base de Dados:** [Yellow Taxi Trip Records (PARQUET)](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)

---

## üó∫Ô∏è Vis√£o Geral das Etapas
O objetivo √© percorrer todo o ciclo de vida dos dados. Abaixo, a lista completa das etapas, com **as realizadas destacadas**:
- ‚úÖ **Etapa 0:** Organiza√ß√£o e Planejamento *(Conclu√≠da)*
- ‚úÖ **Etapa 1:** Definir o Dataset *(Conclu√≠da)*
- ‚úÖ **Etapa 2:** Integrar e pr√©-analisar os dados √† plataforma *(Conclu√≠da)*
- ‚úÖ **Etapa 3:** Explorar os dados seguindo boas pr√°ticas *(Conclu√≠da)*
- ‚úÖ **Etapa 4:** Identificar e produzir relat√≥rio sobre Qualidade dos Dados *(Conclu√≠da)*
- ‚è≥ Etapa 5: Avaliar a capacidade de transforma√ß√£o de dados brutos em features
- ‚è≥ Etapa 6: Propor uma solu√ß√£o de modelagem de dados
- ‚úÖ **Etapa 7:** Analisar os dados na plataforma *(Conclu√≠da)*
- ‚è≥ Etapa 8: Criar uma pipeline para processamento
- ‚è≥ Etapa 9: Criar um DataApp com Streamlit
- ‚è≥ Etapa 10: Apresenta√ß√£o do Case Completo

*Nota: Este documento detalha as etapas conclu√≠das (0, 1, 2, 3, 4 e 7), que comp√µem os requisitos m√≠nimos.*

---

## üìù Relato Detalhado por Etapa

### Etapa 0 - Organiza√ß√£o e Planejamento
> **Contexto & Decis√£o:** Com um prazo de 7 dias, o planejamento foi fundamental para uma entrega consistente.

* Com o prazo de 7 dias para entrega do case t√©cnico, planejamento e organiza√ß√£o s√£o fundamentais para uma entrega completa, consistente e de qualidade.

    Para isso, eu criei um quadro no Jira, neste quadro, eu dividi as tarefas a serem entregues em grupos e subgrupos. Cada grupo, contem um ou mais subgrupos. 
    
    Estes grupos foram organizados de maneira estrat√©gica para permitir a entrega completa e mitigar perdas de tempo ou falhas de planejamento.

    A sua estrutura segue a l√≥gica de avalia√ß√£o, contendo os grupos: m√≠nimo necess√°rio,
                        Intermedi√°rio, 
                        Avan√ßado, 
                        Excelente 
                        e B√¥nus.

    O grupo M√≠nimo aborda as tarefas m√≠nimas para serem avaliadas, o intermedi√°rio aborda as tarefas m√≠nimas + a intermedi√°ria e assim, subsequente, at√© o b√¥nus. *

**Ferramenta Utilizada:** Jira
**Estrutura Criada:** Grupos (M√≠nimo, Intermedi√°rio, Avan√ßado, Excelente, B√¥nus)
**Artefatos Gerados:** Quadro de tarefas (dispon√≠vel na pasta `/docs`)

---

### Etapa 1 - Definir o Dataset
> **Contexto & Decis√£o:** Pivotei de um dataset complexo (`product-search-corpus`) para um mais estruturado (`Yellow-Taxi`), priorizando a viabilidade dentro do cronograma.

*   O processo seletivo iniciou com a tentativa de uso do dataset "product-search-corpus", este dataset √© uma base de dados sobre anuncios de diversos produtos, por√©m organizados de maneira desestruturadas ou semi-estruturadas. O formato do arquivo √© JsonL. 
    
    O obstaculo observado ao utilizar este dataset foi que devido a natureza desestruturada dos dados, a complexidade para realizar a √©tapa de ETL s√©ria muito alto, demandaria um tempo extra para estudo, estrutura√ß√£o, organiza√ß√£o e defini√ß√£o de uma abordagem que, no meu ponto de vista, √© incompativel com o cronograma da entrega.

    A minha decis√£o, foi de pivotar para um dataset mais simples e melhor estruturado, dessa forma, eu escolhi o dataset "Yellow-Taxi-Trip_NY", este dataset √© uma base de dados sobre as vi√°gens de taxi realizadas em New York pela linha amarela. 
    
    Vale ressaltar que ao insistir em um dataset invi√°vel ou com um custo de transforma√ß√£o maior para o tempo e cronograma proposto, pode-se considerar um erro grave de gest√£o de projetos.*

**Dataset Inicial:** `product-search-corpus` (JSONL, semiestruturado)
**Desafio Identificado:** Complexidade de ETL incompat√≠vel com o prazo
**Dataset Escolhido:** `Yellow-Taxi-Trip_NY` (Parquet, bem estruturado)
**Insight Chave:** "Ao insistir em um dataset invi√°vel... pode-se considerar um erro grave de gest√£o de projetos."

---

### Etapa 2 - Integrar e Pr√©-analisar os Dados
> **Contexto & Decis√£o:** Utilizei um script Python para convers√£o e o Google Sheets para integra√ß√£o inicial, o que gerou um aprendizado posterior sobre integridade de dados.

*	Inicialmente, para realizar o processo de integrar e pr√©-analisar os dados escolhidos na plataforma Dadosfera, foi constru√≠do um script em python para converter os dados do tipo .Parquet para o tipo .CSV.

    No script, dispon√≠vel na pasta de dataset, eu inicialmente limitei o arquivo √† 101 mil registros e transformei a base de dados em um dataframe utilizando a biblioteca pandas, ap√≥s, eu converti o dataframe para .csv e exportei o arquivo .csv.

    Ap√≥s de exportado a base de dados no formato .csv, eu importei o arquivo .csv na plataforma Google Sheets, inicialmente, n√£o me apresentou falha ou erro grave, por√©m, ao final, na etapa 7, eu percebi que iria enfrentar mais um obstaculo. Em resumo, o Google Sheets ao tentar "ajudar", aplicou uma formata√ß√£o autom√°tica nos dados e esta formata√ß√£o descaracterizou os tipos dos dados originais, como por exemplo, ao importar a base de dados na plataforma Dadosfera atrav√©s do m√≥dulo de "Conex√£o" e ingest√£o de dados via o m√≥dulo de "Pipelines", algumas colunas que deveriam vir no formato FLOAT ou TIMESTAMP, foram transformadas em VARCHAR (texto).

    Eu identifiquei que a facilidade de integra√ß√£o via ferramenta Sheets pode corromper a integridade de dados e me atrapalhar nas etapas seguintes, por√©m, n√£o me deixei abater por este desafio e perseverei tentando entregar sempre o meu melhor resultado e tentando aprender ou ter uma vis√£o cr√≠tica sobre cada √©tapa.

    A pr√©-analise dos dados ocorreu dentro da plataforma da Dadosfera, no pr√≥prio m√≥dulo de Catalogo, ap√≥s a execu√ß√£o da Pipeline para ingest√£o dos dados na plataforma, √© criado um catalogo sobre estes dados, neste m√≥dulo de catalogo, √© poss√≠vel realizar a etapa de an√°lise descritiva dos dados, como identificar o n√∫mero de linhas, o n√∫mero de colunas, quais s√£o as colunas, o tipo das colunas, uma pr√©via dos dados de cada coluna, al√©m disso pode-se gerar um conjunto de relat√≥rios automaticamente sobre estes dados e analisar a tabela (Ser√° informado na etapa de an√°lise, como ocorreu esta an√°lise).*

**A√ß√£o 1:** Script Python para convers√£o de Parquet para CSV (limite: 101k registros)
**A√ß√£o 2:** Importa√ß√£o do CSV para Google Sheets
**Problema Identificado (Futuro):** Formata√ß√£o autom√°tica do Sheets alterou tipos de dados (ex: FLOAT ‚Üí VARCHAR)
**An√°lise Inicial:** Realizada no cat√°logo da plataforma ap√≥s ingest√£o via pipeline

---

### Etapa 3: Explorar os dados seguindo boas pr√°ticas & 
### Etapa 4:Identificar e produzir relat√≥rio sobre Qualidade dos Dados
> **Contexto & Decis√£o:** Utilizei um framework para valida√ß√£o de qualidade dos dados soda.Scan, o que gerou um aprendizado e experi√™ncia posterior.

*	Para utiliza√ß√£o de uma biblioteca robusta sobre an√°lise e qualidade dos dados, eu tentei implementar o framework Soda para valida√ß√£o de Data Quality. Por√©m, mais uma vez, enfrentei um novo obstaculo, conflitos de dep√™ndencias no pip e restri√ß√µes l√≥gicas no meu ambiente local impediram a plena execu√ß√£o da ferramenta.

    Por√©m, mais uma vez, eu n√£o me deixei abater, assim como um bom profissional n√£o fica apegado √† ferramenta, eu recorri a biblioteca Pandas para tentar identificar poss√≠veis problemas ou inconsist√™ncias nos dados. Eu executei os comandos (df.info() e df.describe()), provando que conhe√ßo da teoria e sei o que se deve validade ou identificar, independente da ferramenta ou biblioteca utilizada. Eu identifiquei valores negativos e inconsist√™ncias nos dados.
    
    √â v√°lido destacar que ap√≥s resultados e identifica√ß√µes de inconsistencias dos dados via pandas, foi-se descartado a possibilidade de uso de ambientes de cloud, como o google colab. Foi uma decis√£o de projeto para mitigar o tempo e a solu√ß√£o entregue, fator importante em um projeto de software.*

**A√ß√£o 1:** Estudar e aplicar o framework soda.Scan
**A√ß√£o 2:** Ap√≥s identificado conflitos de dependencia, foi utilizado a biblioteca pandas
**Problema Identificado (Futuro):** Conflito no ambiente local por conta de dependencias l√≥gicas das bibliotecas pip e soda.Scan
**An√°lise Inicial:** Utilizar a biblioteca pandas atrav√©s dos comandos df.describe() e df.info()
**An√°lise Posterior:** Conforme resultado positivo apartir da an√°lise via pandas, foi-se descartado a possibilidade de execu√ß√£o do c√≥digo em ambiente de cloud, como o google colab.
**Principal Insight:** Na pr√≥xima vez, utilizar um ambiente de cloud como o Google Colab, facilita e mitiga falhas em ambientes locais. 

---

### Etapa 7 - Analisar os Dados na Plataforma
> **Contexto & Decis√£o:** Contornei problemas de qualidade de dados herdados usando SQL no Metabase, focando em gerar valor a partir de dados imperfeitos.

*A pr√≥xima etapa, embora n√£o siga uma sequ√™ncia l√≥gica exata, ela faz parte dos requisitos m√≠nimos do teste.

    na etapa 7, foi utilizado o M√≥dulo de "Visualiza√ß√£o" dos dados, nele, somos redirecionados para a plataforma de BI e visualiza√ß√£o de dados Metabase.

    Foi nesta etapa que a "ajuda" do Google Sheets foi realmente afetada, pois, √© n√≠tido que caso, os dados obtidos possuam inconsist√™ncias ou falhas graves, como a altera√ß√£o dos tipos das vari√°veis, acarretar√° em gr√°ficos incosistentes ou problemas na gera√ß√£o de gr√°ficos com maior qualidade.

    Neste sentido, os dados chegaram como texto (via sheets), o motor do BI n√£o era capaz de extrair ou realizar eixos temporais e soma. Neste momento, veio a incerteza e o sentimento que "nada estava saindo como planejado ou como ideal", considerando inclusive, a falta dos gr√°ficos como falha pessoal. Por√©m, ap√≥s refletir e repensar o cen√°rio, ao inv√©s de come√ßar do zero, eu tentei fazer a manipula√ß√£o na estrutura do Metabase via SQL bruto e corrigindo as inconsist√™ncias.

    Ap√≥s muito pesquisar e estudar, eu elaborei uma query robusta contendo o uso do TRY_TO_DOUBLE para for√ßar as strings em dados n√∫mericos, utilizar do TO_DATE E LEFT para reconstruir a integridade temporal, filtros WHERE para reomver o faturamento negativo identificado via Pandas. O Resultado obtido foi que o dado "quebrado" foi transformado em uma tabela de indicadores executivos, com as colunas calculadas manualmente como "Receita Total", "Dist√¢ncia M√©dia" e "Volume de viagens".

    Honestamente, eu gostaria que nesta etapa fossem criados muitos gr√°ficos, gr√°ficos bonitos, uma dashboard legal, mas dada todas as dificuldades, transtornos e nuances, eu consegui contornar todas elas e construir ainda que apenas em uma tabela, mas que demonstra valor e corrige todas as imperfei√ß√µes e defeitos. Nem sempre teremos o cen√°rio perfeito ou o ambiente perfeito com tudo funcionando, mas saber lidar com cada dificuldade, apoiado no conhecimento t√©cnico e suporte das ferramentas de IA + artigos e posts em blogs dispon√≠veis, faz total diferen√ßa nesta jornada.

    A entrega desta etapa n√£o √© exatamente diversos gr√°ficos e uma dashboard bonita, mas o resultado que persever√£n√ßa, resiliencia e proatividade em busca de uma solu√ß√£o fazem a diferen√ßa.*

**Ferramenta:** M√≥dulo de Visualiza√ß√£o (Metabase)
**Problema Herdado:** Dados chegando como texto (VARCHAR)
**Solu√ß√£o T√©cnica:** Query SQL com `TRY_TO_DOUBLE`, `TO_DATE`, `LEFT`, filtros `WHERE`
**Resultado Final:** Tabela de indicadores executivos com "Receita Total", "Dist√¢ncia M√©dia", "Volume de viagens"
**Principal Insight:** "Nem sempre teremos o cen√°rio perfeito... saber lidar com cada dificuldade... faz total diferen√ßa."

---
